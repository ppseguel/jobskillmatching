{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "industry_script.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppseguel/jobskillmatching/blob/master/industry_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxsIaud5tFfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import dataset\n",
        "#follow this documentation, using Github, https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/ppseguel/jobskillmatching/master/marketingintern_industry.csv'\n",
        "job_data = pd.read_csv(url)\n",
        "#job_data = pd.read_csv(url, sep=',').values\n",
        "#job_data = pd.DataFrame(job_data)\n",
        "#job_data = job_data.values\n",
        "\n",
        "#filling blank cells\n",
        "df = pd.DataFrame(job_data)\n",
        "df1 = df.fillna(0)\n",
        "\n",
        "df1.columns\n",
        "df1 = df1.rename(columns={\"Unnamed: 0\": \"id\", \"Linkedin/bloomberg\":\"LinkedInBloomerg\"})\n",
        "\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "\n",
        "#dataset = p.read_csv('dataset.csv', sep=',').values\n",
        "#dataset = p.DataFrame(dataset)\n",
        "#dataset = dataset.values\n",
        "\n",
        "\n",
        "#from sklearn.preprocessing import Imputer\n",
        "#imputer = Imputer(missing_values='NaN', strategy='mean', axis = 0)\n",
        "#imputer = imputer.fit(job_data[:, 2:6])\n",
        "#job_data[:, 2:6] = imputer.transform(job_data[:, 2:6])\n",
        "#dataset = p.DataFrame(dataset)\n",
        "#dataset = dataset.values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGb1yO7kOfui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#turning textual data to numerical\n",
        "#from sklearn.preprocessing import LabelEncoder\n",
        "#labelencoder_0 = LabelEncoder() #independent variable encoder\n",
        "#job_data[:,0] = labelencoder_0.fit_transform(job_data[:,0])\n",
        "#labelencoder_1 = LabelEncoder() #independent variable encoder\n",
        "#dataset[:,1] = labelencoder_1.fit_transform(job_data[:,1])\n",
        "#labelencoder_6 = LabelEncoder() #dependent (target) variable encoder\n",
        "#job_data[:,6] = labelencoder_6.fit_transform(job_data[:,6])\n",
        "#dataset = p.DataFrame(dataset)\n",
        "#dataset = dataset.values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BTer7m1JbI0-",
        "colab": {}
      },
      "source": [
        "#Checking the dataset\n",
        "#print(job_data)\n",
        "#print(df1)\n",
        "#df1.head()\n",
        "#job_data.head()\n",
        "\n",
        "#print(job_data.Groups, job_data.tableau)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zJTN3J0CVS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EXPERIMENT 1 (EDIT DATASET)\n",
        "\n",
        "# 1. Construct Datasets for Training and Evaluation\n",
        "#selecting y and x variables\n",
        "\n",
        "#splitting the dataset into the source variables (independant variables) and the target variable (dependant variable)\n",
        "#sourcevars = d[[\"agr\",\"art\"] #all columns except the last one\n",
        "#targetvar = df[1] #only the last column\n",
        "\n",
        "sourcevars = df1[df1.columns[-31:-1]] # Remember, Python is 0-offset! The \"3rd\" entry is at slot 2.           \n",
        "targetvar = df1[df1.columns[1:18]] # Remember, Python is 0-offset! The \"3rd\" entry is at slot 2.\n",
        "               \n",
        "X = sourcevars\n",
        "y = targetvar \n",
        "\n",
        "targetvar.head()\n",
        "#print(y)\n",
        "#print(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iog_PexN5JK",
        "colab_type": "code",
        "outputId": "50ed4b2c-e7d0-4f5e-f58d-db8a5b23a24d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# b) Construct Datasets for Training and Evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.25, random_state=42)\n",
        "print(\"Number samples in training: \", len(X_train))\n",
        "print(\"Number samples in testing: \", len(X_test))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number samples in training:  514\n",
            "Number samples in testing:  172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtX_Do-WYTSa",
        "colab_type": "code",
        "outputId": "3103e7b9-d3ce-4155-e91f-87ca1dc3c317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#!pip install -q skmultilearn.adapt\n",
        "!pip install scikit-multilearn\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.6/dist-packages (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opsVYVpdcteD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "1eda0b7d-48d3-4df8-cb41-f43b3d44338c"
      },
      "source": [
        "!pip install git+http://github.com/scikit-learn/scikit-learn.git\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+http://github.com/scikit-learn/scikit-learn.git\n",
            "  Cloning http://github.com/scikit-learn/scikit-learn.git to /tmp/pip-req-build-naucwg78\n",
            "  Running command git clone -q http://github.com/scikit-learn/scikit-learn.git /tmp/pip-req-build-naucwg78\n",
            "Requirement already satisfied (use --upgrade to upgrade): scikit-learn==0.22.dev0 from git+http://github.com/scikit-learn/scikit-learn.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.dev0) (1.16.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.dev0) (1.2.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.dev0) (0.12.5)\n",
            "Building wheels for collected packages: scikit-learn\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qk29wh91/wheels/ec/59/9a/0fba63f83339d1b28888fbe388362cacb251b22521267a64e5\n",
            "Successfully built scikit-learn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzJbj6BTYa-T",
        "colab_type": "code",
        "outputId": "f74095a7-5306-4aa9-d3ba-e71412d862a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#a) Decision Tree Classifier\n",
        "# using classifier chains\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "#from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# initialize classifier chains multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "tree_giniIndex = ClassifierChain(DecisionTreeClassifier(criterion='entropy', max_depth=17, class_weight=\"balanced\"))\n",
        "\n",
        "# train\n",
        "tree_giniIndex.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "y_predict_DT = tree_giniIndex.predict(X_test)\n",
        "\n",
        "#accuracy_DT = accuracy_score(y_test,y_predict_DT)\n",
        "#print(accuracy_DT)\n",
        "\n",
        "f1_score_DT = f1_score(y_test,y_predict_DT, average=None)\n",
        "#f1_score_DT = f1_score(y_test,y_predict_DT, average=\"micro\")\n",
        "#f1_score_DT = f1_score(y_test,y_predict_DT, average=\"macro\")\n",
        "#f1_score_DT = f1_score(y_test,y_predict_DT, average=\"weighted\")\n",
        "#f1_score_DT = f1_score(y_test,y_predict_DT, average=\"binary\")\n",
        "\n",
        "print(f1_score_DT)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.29166667 0.27692308 0.         0.37735849\n",
            " 0.32835821 0.21052632 0.09090909 0.         0.32142857 0.3\n",
            " 0.35294118 0.2745098  0.14814815 0.2962963  0.        ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9mr_U6_ZXc4",
        "colab_type": "code",
        "outputId": "8915b72f-d835-409c-cfcb-dfd8cb578230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#b) KneighborsClassifier\n",
        "# using classifier chains\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# initialize classifier chains multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "Knclassifier = ClassifierChain(KNeighborsClassifier(n_neighbors=1, p =1))\n",
        "\n",
        "# train\n",
        "Knclassifier.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "y_predict_KN = Knclassifier.predict(X_test)\n",
        "\n",
        "accuracy_KN= accuracy_score(y_test,y_predict_KN)\n",
        "\n",
        "print(accuracy_KN)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.22674418604651161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmVwNSJlRATJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree_giniIndex = DecisionTreeClassifier(criterion='entropy', max_depth=6, class_weight=\"balanced\")\n",
        "tree_giniIndex.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O52qMgUNEFmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EXPERIMENT 1 (EDIT DATASET AND TO MULTILABEL)\n",
        "\n",
        "# a) Train and evaluate a decision treel model\n",
        "\n",
        "# using classifier chains\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# initialize classifier chains multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "tree_giniIndex = ClassifierChain(DecisionTreeClassifier(criterion='entropy', max_depth=6, class_weight=\"balanced\"))\n",
        "\n",
        "# train\n",
        "tree_giniIndex.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=2)\n",
        "tree_scores = cross_val_score(tree_giniIndex, X, y, cv=kfold, scoring=\"accuracy\")\n",
        "print(\"Cross-validation score:\\n{}\".format(tree_scores))\n",
        "print(\"Average cross-validation score: {:.2f}\".format(tree_scores.mean()))\n",
        "print(\"Standard deviation: {:.2f}\".format(tree_scores.std()))\n",
        "\n",
        "# Tune Decision Tree hyperparameter using cross-validation\n",
        "best_score = 0\n",
        "for max_d in [1,2,3,4,5,6,7,8,9,11,13,15,17,18,19]:\n",
        "    \n",
        "    tree_giniIndex = DecisionTreeClassifier(max_depth=max_d).fit(X_train, y_train)\n",
        "    fold_accuracies = cross_val_score(tree_giniIndex, X_train, y_train, cv=kfold, scoring=\"accuracy\") \n",
        "    score = fold_accuracies.mean()\n",
        "    \n",
        "    print(\"Score for depth of \" + str(max_d) + \"on validation set is {:0.2f}\".format(score))\n",
        "    \n",
        "    #Best score for best optimal model\n",
        "    \n",
        "    if score > best_score:\n",
        "        best_param = {'max_depth': max_d}\n",
        "        best_score = score\n",
        "        \n",
        "#Part of the answer of question 3        \n",
        "tree_giniIndex = DecisionTreeClassifier(**best_param)\n",
        "tree_giniIndex.fit(X_train, y_train)\n",
        "test_score = tree_giniIndex.score(X_test, y_test)\n",
        "print(\"Best score on cross-validation: {:0.2f}\".format(best_score))\n",
        "print(\"Best parameters: {}\".format(best_param))\n",
        "print(\"Test set score: {:.2f}\".format(test_score))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMXdhZlXEQXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EXPERIMENT 1 (EDIT DATASET AND TO MULTILABEL)\n",
        "\n",
        "\n",
        "# b) Nearest Neighbors (K-NN)\n",
        "# Normalize data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "mms = MinMaxScaler()\n",
        "X_train_norm = mms.fit_transform(X_train)\n",
        "X_test_norm = mms.transform(X_test)\n",
        "\n",
        "# Tune hyperparameters using cross-validation for K-NN\n",
        "\n",
        "\n",
        "#b) KneighborsClassifier\n",
        "# using classifier chains\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=2)\n",
        "\n",
        "best_score = 0\n",
        "\n",
        "for p in [1, 2, 3, 4, 5]:\n",
        "    for n in [1, 2, 3, 4, 5, 6]:   \n",
        "        clf = ClassifierChain(KNeighborsClassifier(n_neighbors=n, p=p, metric='minkowski'))\n",
        "        clf.fit(X_train_norm, y_train)\n",
        "            \n",
        "        fold_accuracies = cross_val_score(clf, X_train, y_train, cv=kfold, scoring=\"accuracy\")  \n",
        "        score = fold_accuracies.mean()\n",
        "        print(\"Score for p \" + str(p) + \", score of neighbors \" + str(n) + \" on validation set is {:0.2f}\".format(score))\n",
        "    \n",
        "        if score > best_score:\n",
        "            best_param = {\"p\": p, \"neighbors\": n}\n",
        "            best_score = score\n",
        "\n",
        "#rebuild the model and evaluate the test score                \n",
        "clf = KNeighborsClassifier(**best_param)\n",
        "clf.fit(X_train_norm, y_train)\n",
        "test_score = clf.score(X_test_norm, y_test)\n",
        "print(\"Best score on cross-validation: {:0.2f}\".format(best_score))\n",
        "print(\"Best parameters: {}\".format(best_param))\n",
        "print(\"Test set score: {:.2f}\".format(test_score))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53yTj4PMOOJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "mms = MinMaxScaler()\n",
        "X_train_norm = mms.fit_transform(X_train)\n",
        "X_test_norm = mms.transform(X_test)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "X_train_scaled = ss.fit_transform(X_train)\n",
        "X_test_scaled = ss.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIgyd8PB8XiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2(b) K-Nearest Neighbors\n",
        "\n",
        "\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "\n",
        "#training_accuracy = []\n",
        "#test_accuracy = []\n",
        "knn_best_score=0\n",
        "neighbor_settings = range(1, 11)\n",
        "distance_metrics = range(1, 3)\n",
        "for n_neighbors in neighbor_settings:\n",
        "    for p in distance_metrics:\n",
        "        knn_clf = ClassifierChain(KNeighborsClassifier(n_neighbors=n_neighbors, p=p))\n",
        "        fold_accuracies = cross_val_score(knn_clf, X_train_norm, y_train, cv=kfold, scoring=\"accuracy\")\n",
        "        score = fold_accuracies.mean()\n",
        "        if score > knn_best_score: \n",
        "            knn_best_score = score\n",
        "            knn_clf_best_parameters={'n_neighbors': n_neighbors,'p': p}\n",
        "        \n",
        "\n",
        "print(\"Average cross-validation score: {:.10f}\".format(knn_best_score))\n",
        "print(\"Best parameter:\", knn_clf_best_parameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSMVkt0lEqZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3 Optimal Decision Tree model predictive performance\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree_giniIndex = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
        "tree_giniIndex.fit(X_train, y_train)\n",
        "\n",
        "y_predictedDT = tree_giniIndex.predict(X_test_scaled)\n",
        "print(metrics.classification_report(y_predictedDT, y_test))\n",
        "print(metrics.accuracy_score(y_predictedDT, y_test))\n",
        "\n",
        "#3 Optimal SVM model predictive performance\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "poly_kernel_svm_clf = SVC(kernel=\"poly\", C = 1, degree=0, gamma=0.1)\n",
        "poly_kernel_svm_clf.fit(X_train_scaled, y_train)\n",
        "y_predictedSVM = poly_kernel_svm_clf.predict(X_test_scaled)\n",
        "print(metrics.classification_report(y_predictedSVM, y_test))\n",
        "print(metrics.accuracy_score(y_predictedSVM, y_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS5QS28NEr9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize confusion matrix for the Naive Bayes\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "mat = confusion_matrix(y_predictedNB, y_test)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label')\n",
        "\n",
        "# Visualize confusion matrix for Decision Tree\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "mat = confusion_matrix(y_predictedDT, y_test)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label')\n",
        "\n",
        "# Visualize confusion matrix for the SVM\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "mat = confusion_matrix(y_predictedSVM, y_test)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmHcxH1mVZT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi0A_RdeFpzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MULTILABEL CONFUSION MATRIX\n",
        "import numpy as np\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "\n",
        "#multilabel_confusion_matrix(y_true, y_pred, sample_weight=None, labels=None, samplewise=False)\n",
        "multilabel_confusion_matrix(y_test, y_predict_DT, sample_weight=None, samplewise=False)\n",
        "\n",
        "\n",
        "#f1_score_DT = f1_score(y_test,y_predict_DT, average=None)\n",
        "\n",
        "#labels=[\"agr\",\"art\",\"cons\",\"corp\",\"edu\",\"fin\",\"good\",\"gov\",\"hlth\",\"leg\",\"man\",\"med\",\"org\",\"rec\",\"serv\",\"tech\",\"tran\"],"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAZleTMVCbvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EXPERIMENT 2\n",
        "# Plot learning curves\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "    train_errors, test_errors = [], []\n",
        "   # change de len for m in range(1, len(x_train)):\n",
        "    for m in range(1, 5):\n",
        "        model.fit(x_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(x_train[:m])\n",
        "        y_test_predict = model.predict(x_test)\n",
        "        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))\n",
        "        test_errors.append(mean_squared_error(y_test_predict, y_test))\n",
        "        plt.plot(train_errors, \"r-+\", linewidth = 2, label = \"Training Data\")\n",
        "        plt.plot(test_errors, \"b-+\", linewidth = 3, label = \"Test Data\")\n",
        "        plt.ylabel(\"Mean squared error\")\n",
        "        plt.title(\"Learning Curves\")\n",
        "        plt.legend()\n",
        "        \n",
        "linear_reg_model = linear_model.LinearRegression()\n",
        "plot_learning_curves(linear_reg_model, x_real, y_real)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b48EpEXJjGQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tree_giniIndex\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import coverage_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    train_errors, test_errors = [], []\n",
        "    for m in range(1, 1280, 30):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_test_predict = model.predict(X_test)\n",
        "        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))\n",
        "        test_errors.append(mean_squared_error(y_test_predict, y_test))\n",
        "        plt.plot(train_errors, \"r-\", linewidth = 1)\n",
        "        plt.plot(test_errors, \"b-\", linewidth = 1)\n",
        "        plt.ylabel('Mean squared error')\n",
        "        plt.title('Linear_Learning Curves')\n",
        "        plt.ylim(0, 5)\n",
        "\n",
        "linear_reg_model = linear_model.LinearRegression()\n",
        "plot_learning_curves(linear_reg_model, X, y_withNoise)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f5hqRO55Eaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "    train_errors, test_errors = [], []\n",
        "    for m in range(1, 570, 10):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_test_predict = model.predict(X_test)\n",
        "        train_errors.append(f1_score(y_train_predict, y_train[:m], average=\"weighted\"))\n",
        "        test_errors.append(f1_score(y_test_predict, y_test,  average=\"weighted\"))\n",
        "        plt.plot(train_errors, \"r-\", linewidth = 1)\n",
        "        plt.plot(test_errors, \"b-\", linewidth = 1)\n",
        "        plt.ylabel('f1_score')\n",
        "        plt.title('Tree_giniIndex_Learning Curves')\n",
        "        plt.ylim(0, 2)\n",
        "\n",
        "tree_giniIndex = ClassifierChain(DecisionTreeClassifier(criterion='entropy', max_depth=15, class_weight=\"balanced\"))\n",
        "plot_learning_curves(tree_giniIndex, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wQHUkeV5Glq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "    train_errors, test_errors = [], []\n",
        "    for m in range(1, 570, 10):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_test_predict = model.predict(X_test)\n",
        "        train_errors.append(f1_score(y_train_predict, y_train[:m], average=\"weighted\"))\n",
        "        test_errors.append(f1_score(y_test_predict, y_test,  average=\"weighted\"))\n",
        "        plt.plot(train_errors, \"r-\", linewidth = 1)\n",
        "        plt.plot(test_errors, \"b-\", linewidth = 1)\n",
        "        plt.ylabel('f1_score')\n",
        "        plt.title('Tree_giniIndex_Learning Curves')\n",
        "        plt.ylim(0, 2)\n",
        "\n",
        "tree_giniIndex = ClassifierChain(DecisionTreeClassifier(criterion='entropy', max_depth=15, class_weight=\"balanced\"))\n",
        "plot_learning_curves(tree_giniIndex, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh3ovc9h5MG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import label_ranking_loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "    train_errors, test_errors = [], []\n",
        "    for m in range(1, 570, 10):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_test_predict = model.predict(X_test)\n",
        "        train_errors.append(label_ranking_loss(y_train_predict, y_train[:m]))\n",
        "        test_errors.append(label_ranking_loss(y_test_predict, y_test))\n",
        "        plt.plot(train_errors, \"r-\", linewidth = 1)\n",
        "        plt.plot(test_errors, \"b-\", linewidth = 1)\n",
        "        plt.ylabel('label_ranking_loss')\n",
        "        plt.title('Tree_giniIndex_Learning Curves')\n",
        "        plt.ylim(0, 2)\n",
        "\n",
        "tree_giniIndex = ClassifierChain(DecisionTreeClassifier(criterion='entropy', max_depth=15, class_weight=\"balanced\"))\n",
        "plot_learning_curves(tree_giniIndex, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyDqg94G5Rn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import label_ranking_average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "    train_errors, test_errors = [], []\n",
        "    for m in range(1, 570, 10):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_test_predict = model.predict(X_test)\n",
        "        train_errors.append(label_ranking_average_precision_score(y_train_predict, y_train[:m]))\n",
        "        test_errors.append(label_ranking_average_precision_score(y_test_predict, y_test))\n",
        "        plt.plot(train_errors, \"r-\", linewidth = 1)\n",
        "        plt.plot(test_errors, \"b-\", linewidth = 1)\n",
        "        plt.ylabel('label_ranking_average_precision_score')\n",
        "        plt.title('Tree_giniIndex_Learning Curves')\n",
        "        plt.ylim(0, 2)\n",
        "\n",
        "tree_giniIndex = ClassifierChain(DecisionTreeClassifier(criterion='entropy', max_depth=15, class_weight=\"balanced\"))\n",
        "plot_learning_curves(tree_giniIndex, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGvGtvZL5YKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#b) KneighborsClassifier\n",
        "# using classifier chains\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# initialize classifier chains multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "Knclassifier = ClassifierChain(KNeighborsClassifier())\n",
        "\n",
        "# train\n",
        "Knclassifier.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "y_predict_KN = Knclassifier.predict(X_test)\n",
        "\n",
        "accuracy_KN= accuracy_score(y_test,y_predict_KN)\n",
        "\n",
        "print(accuracy_KN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy5aJgyo5Y4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "426hEGx5bhpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using skmultilearn\n",
        "\n",
        "from skmultilearn.adapt import MLkNN\n",
        "\n",
        "classifier = MLkNN(k=20)\n",
        "\n",
        "# train\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "accuracy_score(y_test,predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}