{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "industry_script.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppseguel/jobskillmatching/blob/master/industry_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxsIaud5tFfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import dataset\n",
        "#follow this documentation, using Github, https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/ppseguel/jobskillmatching/master/marketingintern_industry.csv'\n",
        "job_data = pd.read_csv(url)\n",
        "#job_data = pd.read_csv(url, sep=',').values\n",
        "#job_data = pd.DataFrame(job_data)\n",
        "#job_data = job_data.values\n",
        "\n",
        "#filling blank cells\n",
        "df = pd.DataFrame(job_data)\n",
        "df1 = df.fillna(0)\n",
        "\n",
        "df1.columns\n",
        "df1 = df1.rename(columns={\"Unnamed: 0\": \"id\", \"Linkedin/bloomberg\":\"LinkedInBloomerg\"})\n",
        "\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "\n",
        "#dataset = p.read_csv('dataset.csv', sep=',').values\n",
        "#dataset = p.DataFrame(dataset)\n",
        "#dataset = dataset.values\n",
        "\n",
        "\n",
        "#from sklearn.preprocessing import Imputer\n",
        "#imputer = Imputer(missing_values='NaN', strategy='mean', axis = 0)\n",
        "#imputer = imputer.fit(job_data[:, 2:6])\n",
        "#job_data[:, 2:6] = imputer.transform(job_data[:, 2:6])\n",
        "#dataset = p.DataFrame(dataset)\n",
        "#dataset = dataset.values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGb1yO7kOfui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#turning textual data to numerical\n",
        "#from sklearn.preprocessing import LabelEncoder\n",
        "#labelencoder_0 = LabelEncoder() #independent variable encoder\n",
        "#job_data[:,0] = labelencoder_0.fit_transform(job_data[:,0])\n",
        "#labelencoder_1 = LabelEncoder() #independent variable encoder\n",
        "#dataset[:,1] = labelencoder_1.fit_transform(job_data[:,1])\n",
        "#labelencoder_6 = LabelEncoder() #dependent (target) variable encoder\n",
        "#job_data[:,6] = labelencoder_6.fit_transform(job_data[:,6])\n",
        "#dataset = p.DataFrame(dataset)\n",
        "#dataset = dataset.values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BTer7m1JbI0-",
        "colab": {}
      },
      "source": [
        "#Checking the dataset\n",
        "#print(job_data)\n",
        "#print(df1)\n",
        "#df1.head()\n",
        "#job_data.head()\n",
        "\n",
        "#print(job_data.Groups, job_data.tableau)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zJTN3J0CVS_",
        "colab_type": "code",
        "outputId": "5e04cfa8-cef2-47cd-f005-d0cebb06a0e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2176
        }
      },
      "source": [
        "#EXPERIMENT 1 (EDIT DATASET)\n",
        "\n",
        "# 1. Construct Datasets for Training and Evaluation\n",
        "#selecting y and x variables\n",
        "\n",
        "#splitting the dataset into the source variables (independant variables) and the target variable (dependant variable)\n",
        "#sourcevars = d[[\"agr\",\"art\"] #all columns except the last one\n",
        "#targetvar = df[1] #only the last column\n",
        "\n",
        "sourcevars = df1[df1.columns[-31:-1]] # Remember, Python is 0-offset! The \"3rd\" entry is at slot 2.           \n",
        "targetvar = df1[df1.columns[1:18]] # Remember, Python is 0-offset! The \"3rd\" entry is at slot 2.\n",
        "               \n",
        "X = sourcevars\n",
        "y = targetvar \n",
        "\n",
        "#targetvar.head()\n",
        "print(y)\n",
        "#print(X)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     agr  art  cons  corp  edu  fin  good  gov  hlth  leg  man  med  org  rec  \\\n",
            "0      1    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "1      1    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "2      0    1     0     0    0    0     0    0     0    0    0    1    0    0   \n",
            "3      0    1     0     0    0    0     0    0     0    0    0    1    0    0   \n",
            "4      0    1     0     0    0    0     0    0     0    0    0    1    0    0   \n",
            "5      0    1     0     0    0    0     0    0     0    0    0    1    0    0   \n",
            "6      0    1     0     0    0    0     0    0     0    0    0    1    0    1   \n",
            "7      0    1     0     0    0    0     0    0     0    0    0    1    0    1   \n",
            "8      0    1     0     0    0    0     0    0     0    0    0    1    0    1   \n",
            "9      0    1     0     0    0    0     0    0     0    0    0    1    0    1   \n",
            "10     0    1     0     0    0    0     0    0     0    0    0    1    0    1   \n",
            "11     0    1     0     0    0    0     0    0     0    0    0    1    0    1   \n",
            "12     0    1     0     0    0    0     0    0     0    0    0    1    0    1   \n",
            "13     0    1     0     0    0    0     0    0     0    0    0    0    0    1   \n",
            "14     0    1     0     0    0    0     0    0     0    0    0    0    0    1   \n",
            "15     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "16     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "17     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "18     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "19     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "20     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "21     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "22     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "23     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "24     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "25     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "26     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "27     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "28     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "29     0    0     1     0    0    0     0    0     0    0    0    0    0    0   \n",
            "..   ...  ...   ...   ...  ...  ...   ...  ...   ...  ...  ...  ...  ...  ...   \n",
            "656    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "657    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "658    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "659    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "660    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "661    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "662    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "663    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "664    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "665    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "666    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "667    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "668    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "669    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "670    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "671    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "672    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "673    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "674    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "675    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "676    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "677    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "678    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "679    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "680    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "681    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "682    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "683    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "684    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "685    0    0     0     0    0    0     0    0     0    0    0    0    0    0   \n",
            "\n",
            "     serv  tech  tran  \n",
            "0       0     0     0  \n",
            "1       0     0     0  \n",
            "2       0     0     0  \n",
            "3       0     0     0  \n",
            "4       0     0     0  \n",
            "5       0     0     0  \n",
            "6       0     0     0  \n",
            "7       0     0     0  \n",
            "8       0     0     0  \n",
            "9       0     0     0  \n",
            "10      0     0     0  \n",
            "11      0     0     0  \n",
            "12      0     0     0  \n",
            "13      0     0     0  \n",
            "14      0     0     0  \n",
            "15      0     0     0  \n",
            "16      0     0     0  \n",
            "17      0     0     0  \n",
            "18      0     0     0  \n",
            "19      0     0     0  \n",
            "20      0     0     0  \n",
            "21      0     0     0  \n",
            "22      0     0     0  \n",
            "23      0     0     0  \n",
            "24      0     0     0  \n",
            "25      0     0     0  \n",
            "26      0     0     0  \n",
            "27      0     0     0  \n",
            "28      0     0     0  \n",
            "29      0     0     0  \n",
            "..    ...   ...   ...  \n",
            "656     0     1     0  \n",
            "657     0     1     0  \n",
            "658     0     1     0  \n",
            "659     0     1     0  \n",
            "660     0     1     0  \n",
            "661     0     1     0  \n",
            "662     0     1     0  \n",
            "663     0     1     0  \n",
            "664     0     1     0  \n",
            "665     0     1     0  \n",
            "666     0     1     0  \n",
            "667     0     1     0  \n",
            "668     0     1     0  \n",
            "669     0     1     0  \n",
            "670     0     1     0  \n",
            "671     0     1     0  \n",
            "672     0     1     0  \n",
            "673     0     1     0  \n",
            "674     0     1     0  \n",
            "675     0     1     0  \n",
            "676     0     1     0  \n",
            "677     0     1     0  \n",
            "678     0     1     0  \n",
            "679     0     1     0  \n",
            "680     0     1     0  \n",
            "681     0     0     1  \n",
            "682     0     0     1  \n",
            "683     0     0     1  \n",
            "684     0     0     1  \n",
            "685     0     0     1  \n",
            "\n",
            "[686 rows x 17 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iog_PexN5JK",
        "colab_type": "code",
        "outputId": "6fc9ab13-1e38-476d-db3f-cbd151313b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# b) Construct Datasets for Training and Evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.25, random_state=42)\n",
        "print(\"Number samples in training: \", len(X_train))\n",
        "print(\"Number samples in testing: \", len(X_test))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number samples in training:  514\n",
            "Number samples in testing:  172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtX_Do-WYTSa",
        "colab_type": "code",
        "outputId": "b383121a-a2c7-48be-b900-6ef857d2e343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#!pip install -q skmultilearn.adapt\n",
        "!pip install scikit-multilearn\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-multilearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/1f/e6ff649c72a1cdf2c7a1d31eb21705110ce1c5d3e7e26b2cc300e1637272/scikit_multilearn-0.2.0-py3-none-any.whl (89kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.0MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opsVYVpdcteD",
        "colab_type": "code",
        "outputId": "8b7427c4-8c76-45eb-9f60-c1afe0063ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!pip install git+http://github.com/scikit-learn/scikit-learn.git\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+http://github.com/scikit-learn/scikit-learn.git\n",
            "  Cloning http://github.com/scikit-learn/scikit-learn.git to /tmp/pip-req-build-pgbeq6oy\n",
            "  Running command git clone -q http://github.com/scikit-learn/scikit-learn.git /tmp/pip-req-build-pgbeq6oy\n",
            "Requirement already satisfied (use --upgrade to upgrade): scikit-learn==0.22.dev0 from git+http://github.com/scikit-learn/scikit-learn.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.dev0) (1.16.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.dev0) (1.2.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.dev0) (0.12.5)\n",
            "Building wheels for collected packages: scikit-learn\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-aidwqhaw/wheels/ec/59/9a/0fba63f83339d1b28888fbe388362cacb251b22521267a64e5\n",
            "Successfully built scikit-learn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzJbj6BTYa-T",
        "colab_type": "code",
        "outputId": "e54ed558-6ef7-4fd8-a914-4172a3da2edc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#a) Decision Tree Classifier\n",
        "# using classifier chains\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "#from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# initialize classifier chains multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "tree_giniIndex = ClassifierChain(DecisionTreeClassifier(criterion='entropy', max_depth=18, class_weight=\"balanced\"))\n",
        "\n",
        "# train\n",
        "tree_giniIndex.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "y_predict_DT = tree_giniIndex.predict(X_test)\n",
        "\n",
        "#accuracy_DT = accuracy_score(y_test,y_predict_DT)\n",
        "#print(accuracy_DT)\n",
        "\n",
        "f1_score_DT = f1_score(y_test,y_predict_DT, average=None)\n",
        "#f1_score_DT = f1_score(y_test,y_predict_DT, average=\"micro\")\n",
        "#f1_score_DT = f1_score(y_test,y_predict_DT, average=\"macro\")\n",
        "#f1_score_DT = f1_score(y_test,y_predict_DT, average=\"weighted\")\n",
        "#f1_score_DT = f1_score(y_test,y_predict_DT, average=\"binary\")\n",
        "\n",
        "print(f1_score_DT)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.34042553 0.29032258 0.         0.43636364\n",
            " 0.25714286 0.22222222 0.09090909 0.         0.32142857 0.17857143\n",
            " 0.25       0.26923077 0.13793103 0.25925926 0.        ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9mr_U6_ZXc4",
        "colab_type": "code",
        "outputId": "025d497a-9dad-4d0f-a96d-7d8fd7523b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#b) KneighborsClassifier\n",
        "# using classifier chains\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# initialize classifier chains multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "Knclassifier = ClassifierChain(KNeighborsClassifier(n_neighbors=1, p =1))\n",
        "\n",
        "# train\n",
        "Knclassifier.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "y_predict_KN = Knclassifier.predict(X_test)\n",
        "\n",
        "accuracy_KN= accuracy_score(y_test,y_predict_KN)\n",
        "\n",
        "print(accuracy_KN)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.22674418604651161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmVwNSJlRATJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree_giniIndex = DecisionTreeClassifier(criterion='entropy', max_depth=6, class_weight=\"balanced\")\n",
        "tree_giniIndex.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O52qMgUNEFmT",
        "colab_type": "code",
        "outputId": "e4089a3c-e754-41a4-a529-76931aae93e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "#EXPERIMENT 1 (EDIT DATASET AND TO MULTILABEL)\n",
        "\n",
        "# a) Train and evaluate a decision treel model\n",
        "\n",
        "# using classifier chains\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# initialize classifier chains multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "tree_giniIndex = ClassifierChain(DecisionTreeClassifier(criterion='gini', max_depth=6, class_weight=\"balanced\"))\n",
        "\n",
        "# train\n",
        "tree_giniIndex.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=2)\n",
        "tree_scores = cross_val_score(tree_giniIndex, X, y, cv=kfold, scoring=\"accuracy\")\n",
        "print(\"Cross-validation score:\\n{}\".format(tree_scores))\n",
        "print(\"Average cross-validation score: {:.2f}\".format(tree_scores.mean()))\n",
        "print(\"Standard deviation: {:.2f}\".format(tree_scores.std()))\n",
        "\n",
        "# Tune Decision Tree hyperparameter using cross-validation\n",
        "best_score = 0\n",
        "for max_d in [1,2,3,4,5,6,7,8,9,11,13,15,17,18,19]:\n",
        "    \n",
        "    tree_giniIndex = DecisionTreeClassifier(max_depth=max_d).fit(X_train, y_train)\n",
        "    fold_accuracies = cross_val_score(tree_giniIndex, X_train, y_train, cv=kfold, scoring=\"accuracy\") \n",
        "    score = fold_accuracies.mean()\n",
        "    \n",
        "    print(\"Score for depth of \" + str(max_d) + \"on validation set is {:0.2f}\".format(score))\n",
        "    \n",
        "    #Best score for best optimal model\n",
        "    \n",
        "    if score > best_score:\n",
        "        best_param = {'max_depth': max_d}\n",
        "        best_score = score\n",
        "        \n",
        "#Part of the answer of question 3        \n",
        "tree_giniIndex = DecisionTreeClassifier(**best_param)\n",
        "tree_giniIndex.fit(X_train, y_train)\n",
        "test_score = tree_giniIndex.score(X_test, y_test)\n",
        "print(\"Best score on cross-validation: {:0.2f}\".format(best_score))\n",
        "print(\"Best parameters: {}\".format(best_param))\n",
        "print(\"Test set score: {:.2f}\".format(test_score))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validation score:\n",
            "[0.         0.02898551 0.07246377 0.01449275 0.01449275 0.02898551\n",
            " 0.02941176 0.07352941 0.07352941 0.02941176]\n",
            "Average cross-validation score: 0.04\n",
            "Standard deviation: 0.03\n",
            "Score for depth of 1on validation set is 0.00\n",
            "Score for depth of 2on validation set is 0.00\n",
            "Score for depth of 3on validation set is 0.00\n",
            "Score for depth of 4on validation set is 0.00\n",
            "Score for depth of 5on validation set is 0.01\n",
            "Score for depth of 6on validation set is 0.03\n",
            "Score for depth of 7on validation set is 0.04\n",
            "Score for depth of 8on validation set is 0.04\n",
            "Score for depth of 9on validation set is 0.07\n",
            "Score for depth of 11on validation set is 0.10\n",
            "Score for depth of 13on validation set is 0.11\n",
            "Score for depth of 15on validation set is 0.12\n",
            "Score for depth of 17on validation set is 0.13\n",
            "Score for depth of 18on validation set is 0.14\n",
            "Score for depth of 19on validation set is 0.13\n",
            "Best score on cross-validation: 0.14\n",
            "Best parameters: {'max_depth': 18}\n",
            "Test set score: 0.20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMXdhZlXEQXt",
        "colab_type": "code",
        "outputId": "61d4f717-f6dc-4d77-ac78-ac277f7fb5d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        }
      },
      "source": [
        "#EXPERIMENT 1 (EDIT DATASET AND TO MULTILABEL)\n",
        "\n",
        "\n",
        "# b) Nearest Neighbors (K-NN)\n",
        "# Normalize data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "mms = MinMaxScaler()\n",
        "X_train_norm = mms.fit_transform(X_train)\n",
        "X_test_norm = mms.transform(X_test)\n",
        "\n",
        "# Tune hyperparameters using cross-validation for K-NN\n",
        "\n",
        "\n",
        "#b) KneighborsClassifier\n",
        "# using classifier chains\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=2)\n",
        "\n",
        "best_score = 0\n",
        "\n",
        "for p in [1, 2, 3, 4, 5]:\n",
        "    for n in [1, 2, 3, 4, 5, 6]:   \n",
        "        clf = ClassifierChain(KNeighborsClassifier(n_neighbors=n, p=p, metric='minkowski'))\n",
        "        clf.fit(X_train_norm, y_train)\n",
        "            \n",
        "        fold_accuracies = cross_val_score(clf, X_train, y_train, cv=kfold, scoring=\"accuracy\")  \n",
        "        score = fold_accuracies.mean()\n",
        "        print(\"Score for p \" + str(p) + \", score of neighbors \" + str(n) + \" on validation set is {:0.2f}\".format(score))\n",
        "    \n",
        "        if score > best_score:\n",
        "            best_param = {\"p\": p, \"neighbors\": n}\n",
        "            best_score = score\n",
        "\n",
        "#rebuild the model and evaluate the test score                \n",
        "clf = KNeighborsClassifier(**best_param)\n",
        "clf.fit(X_train_norm, y_train)\n",
        "test_score = clf.score(X_test_norm, y_test)\n",
        "print(\"Best score on cross-validation: {:0.2f}\".format(best_score))\n",
        "print(\"Best parameters: {}\".format(best_param))\n",
        "print(\"Test set score: {:.2f}\".format(test_score))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score for p 1, score of neighbors 1 on validation set is 0.16\n",
            "Score for p 1, score of neighbors 2 on validation set is 0.06\n",
            "Score for p 1, score of neighbors 3 on validation set is 0.11\n",
            "Score for p 1, score of neighbors 4 on validation set is 0.06\n",
            "Score for p 1, score of neighbors 5 on validation set is 0.10\n",
            "Score for p 1, score of neighbors 6 on validation set is 0.07\n",
            "Score for p 2, score of neighbors 1 on validation set is 0.16\n",
            "Score for p 2, score of neighbors 2 on validation set is 0.06\n",
            "Score for p 2, score of neighbors 3 on validation set is 0.11\n",
            "Score for p 2, score of neighbors 4 on validation set is 0.06\n",
            "Score for p 2, score of neighbors 5 on validation set is 0.10\n",
            "Score for p 2, score of neighbors 6 on validation set is 0.07\n",
            "Score for p 3, score of neighbors 1 on validation set is 0.16\n",
            "Score for p 3, score of neighbors 2 on validation set is 0.06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-07a58c555092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mfold_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score for p \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", score of neighbors \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" on validation set is {:0.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    387\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 231\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# _score will return dict if is_multimetric is True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \"\"\"\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[0;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \"\"\"\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skmultilearn/problem_transform/cc.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             prediction = self.classifiers_[label].predict(\n\u001b[0;32m--> 179\u001b[0;31m                 self._ensure_input_format(X_extended))\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_multi_label_from_single_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mX_extended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_extended\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 delayed_query(\n\u001b[1;32m    453\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 454\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m             )\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 261\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36m_tree_query_parallel_helper\u001b[0;34m(tree, data, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \"\"\"\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53yTj4PMOOJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "mms = MinMaxScaler()\n",
        "X_train_norm = mms.fit_transform(X_train)\n",
        "X_test_norm = mms.transform(X_test)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "X_train_scaled = ss.fit_transform(X_train)\n",
        "X_test_scaled = ss.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIgyd8PB8XiC",
        "colab_type": "code",
        "outputId": "3eaa6989-eead-463c-f223-b7ccb0dbc0b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#2(b) K-Nearest Neighbors\n",
        "\n",
        "\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "\n",
        "#training_accuracy = []\n",
        "#test_accuracy = []\n",
        "knn_best_score=0\n",
        "neighbor_settings = range(1, 11)\n",
        "distance_metrics = range(1, 3)\n",
        "for n_neighbors in neighbor_settings:\n",
        "    for p in distance_metrics:\n",
        "        knn_clf = ClassifierChain(KNeighborsClassifier(n_neighbors=n_neighbors, p=p))\n",
        "        fold_accuracies = cross_val_score(knn_clf, X_train_norm, y_train, cv=kfold, scoring=\"accuracy\")\n",
        "        score = fold_accuracies.mean()\n",
        "        if score > knn_best_score: \n",
        "            knn_best_score = score\n",
        "            knn_clf_best_parameters={'n_neighbors': n_neighbors,'p': p}\n",
        "        \n",
        "\n",
        "print(\"Average cross-validation score: {:.10f}\".format(knn_best_score))\n",
        "print(\"Best parameter:\", knn_clf_best_parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average cross-validation score: 0.1790346908\n",
            "Best parameter: {'n_neighbors': 1, 'p': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSMVkt0lEqZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3 Optimal Decision Tree model predictive performance\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree_giniIndex = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
        "tree_giniIndex.fit(X_train, y_train)\n",
        "\n",
        "y_predictedDT = tree_giniIndex.predict(X_test_scaled)\n",
        "print(metrics.classification_report(y_predictedDT, y_test))\n",
        "print(metrics.accuracy_score(y_predictedDT, y_test))\n",
        "\n",
        "#3 Optimal SVM model predictive performance\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "poly_kernel_svm_clf = SVC(kernel=\"poly\", C = 1, degree=0, gamma=0.1)\n",
        "poly_kernel_svm_clf.fit(X_train_scaled, y_train)\n",
        "y_predictedSVM = poly_kernel_svm_clf.predict(X_test_scaled)\n",
        "print(metrics.classification_report(y_predictedSVM, y_test))\n",
        "print(metrics.accuracy_score(y_predictedSVM, y_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS5QS28NEr9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize confusion matrix for the Naive Bayes\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "mat = confusion_matrix(y_predictedNB, y_test)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label')\n",
        "\n",
        "# Visualize confusion matrix for Decision Tree\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "mat = confusion_matrix(y_predictedDT, y_test)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label')\n",
        "\n",
        "# Visualize confusion matrix for the SVM\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "mat = confusion_matrix(y_predictedSVM, y_test)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmHcxH1mVZT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi0A_RdeFpzO",
        "colab_type": "code",
        "outputId": "77a50506-83bb-47ca-bf5c-e6e65a4ac06a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "#MULTILABEL CONFUSION MATRIX\n",
        "import numpy as np\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "\n",
        "#multilabel_confusion_matrix(y_true, y_pred, sample_weight=None, labels=None, samplewise=False)\n",
        "cm = multilabel_confusion_matrix(y_test, y_predict_DT, sample_weight=None, samplewise=False)\n",
        "\n",
        "print(cm)\n",
        "\n",
        "#[[TN, FP], \n",
        "# [FN,TP]], ith True on the left (Neg, Pos) and predict on the top (Neg, Pos)\n",
        "\n",
        "#f1_score_DT = f1_score(y_test,y_predict_DT, average=None)\n",
        "\n",
        "#labels=[\"agr\",\"art\",\"cons\",\"corp\",\"edu\",\"fin\",\"good\",\"gov\",\"hlth\",\"leg\",\"man\",\"med\",\"org\",\"rec\",\"serv\",\"tech\",\"tran\"],"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-dd4881709d36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultilabel_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#multilabel_confusion_matrix(y_true, y_pred, sample_weight=None, labels=None, samplewise=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'multilabel_confusion_matrix'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hky3NQ80N9HH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    #cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AInk9HGMkMpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting a confusion matrix\n",
        "\n",
        "\n",
        "Confusion matrix, without normalization\n",
        "[[13  0  0]\n",
        " [ 0 10  6]\n",
        " [ 0  0  9]]\n",
        "Normalized confusion matrix\n",
        "[[1.   0.   0.  ]\n",
        " [0.   0.62 0.38]\n",
        " [0.   0.   1.  ]]\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "# import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Split the data into a training set and a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# Run classifier, using a model that is too regularized (C too low) to see\n",
        "# the impact on the results\n",
        "classifier = svm.SVC(kernel='linear', C=0.01)\n",
        "y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAZleTMVCbvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EXPERIMENT 2\n",
        "# Plot learning curves\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "    train_errors, test_errors = [], []\n",
        "   # change de len for m in range(1, len(x_train)):\n",
        "    for m in range(1, 5):\n",
        "        model.fit(x_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(x_train[:m])\n",
        "        y_test_predict = model.predict(x_test)\n",
        "        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))\n",
        "        test_errors.append(mean_squared_error(y_test_predict, y_test))\n",
        "        plt.plot(train_errors, \"r-+\", linewidth = 2, label = \"Training Data\")\n",
        "        plt.plot(test_errors, \"b-+\", linewidth = 3, label = \"Test Data\")\n",
        "        plt.ylabel(\"Mean squared error\")\n",
        "        plt.title(\"Learning Curves\")\n",
        "        plt.legend()\n",
        "        \n",
        "linear_reg_model = linear_model.LinearRegression()\n",
        "plot_learning_curves(linear_reg_model, x_real, y_real)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b48EpEXJjGQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tree_giniIndex\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import coverage_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    train_errors, test_errors = [], []\n",
        "    for m in range(1, 1280, 30):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_test_predict = model.predict(X_test)\n",
        "        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))\n",
        "        test_errors.append(mean_squared_error(y_test_predict, y_test))\n",
        "        plt.plot(train_errors, \"r-\", linewidth = 1)\n",
        "        plt.plot(test_errors, \"b-\", linewidth = 1)\n",
        "        plt.ylabel('Mean squared error')\n",
        "        plt.title('Linear_Learning Curves')\n",
        "        plt.ylim(0, 5)\n",
        "\n",
        "linear_reg_model = linear_model.LinearRegression()\n",
        "plot_learning_curves(linear_reg_model, X, y_withNoise)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f5hqRO55Eaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "    train_errors, test_errors = [], []\n",
        "    for m in range(1, 570, 10):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_test_predict = model.predict(X_test)\n",
        "        train_errors.append(f1_score(y_train_predict, y_train[:m], average=\"weighted\"))\n",
        "        test_errors.append(f1_score(y_test_predict, y_test,  average=\"weighted\"))\n",
        "        plt.plot(train_errors, \"r-\", linewidth = 1)\n",
        "        plt.plot(test_errors, \"b-\", linewidth = 1)\n",
        "        plt.ylabel('f1_score')\n",
        "        plt.title('Tree_giniIndex_Learning Curves')\n",
        "        plt.ylim(0, 2)\n",
        "\n",
        "tree_giniIndex = ClassifierChain(DecisionTreeClassifier(criterion='entropy', max_depth=15, class_weight=\"balanced\"))\n",
        "plot_learning_curves(tree_giniIndex, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wQHUkeV5Glq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "    train_errors, test_errors = [], []\n",
        "    for m in range(1, 570, 10):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_test_predict = model.predict(X_test)\n",
        "        train_errors.append(f1_score(y_train_predict, y_train[:m], average=\"weighted\"))\n",
        "        test_errors.append(f1_score(y_test_predict, y_test,  average=\"weighted\"))\n",
        "        plt.plot(train_errors, \"r-\", linewidth = 1)\n",
        "        plt.plot(test_errors, \"b-\", linewidth = 1)\n",
        "        plt.ylabel('f1_score')\n",
        "        plt.title('Tree_giniIndex_Learning Curves')\n",
        "        plt.ylim(0, 2)\n",
        "\n",
        "tree_giniIndex = ClassifierChain(DecisionTreeClassifier(criterion='entropy', max_depth=15, class_weight=\"balanced\"))\n",
        "plot_learning_curves(tree_giniIndex, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh3ovc9h5MG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import label_ranking_loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "    train_errors, test_errors = [], []\n",
        "    for m in range(1, 570, 10):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_test_predict = model.predict(X_test)\n",
        "        train_errors.append(label_ranking_loss(y_train_predict, y_train[:m]))\n",
        "        test_errors.append(label_ranking_loss(y_test_predict, y_test))\n",
        "        plt.plot(train_errors, \"r-\", linewidth = 1)\n",
        "        plt.plot(test_errors, \"b-\", linewidth = 1)\n",
        "        plt.ylabel('label_ranking_loss')\n",
        "        plt.title('Tree_giniIndex_Learning Curves')\n",
        "        plt.ylim(0, 2)\n",
        "\n",
        "tree_giniIndex = ClassifierChain(DecisionTreeClassifier(criterion='entropy', max_depth=15, class_weight=\"balanced\"))\n",
        "plot_learning_curves(tree_giniIndex, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyDqg94G5Rn3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "43786a3d-4434-4b05-b17e-e97a7fe3bc1a"
      },
      "source": [
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import label_ranking_average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "    train_errors, test_errors = [], []\n",
        "    for m in range(1, 570, 10):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_test_predict = model.predict(X_test)\n",
        "        train_errors.append(label_ranking_average_precision_score(y_train_predict, y_train[:m]))\n",
        "        test_errors.append(label_ranking_average_precision_score(y_test_predict, y_test))\n",
        "        plt.plot(train_errors, \"r-\", linewidth = 1)\n",
        "        plt.plot(test_errors, \"b-\", linewidth = 1)\n",
        "        plt.ylabel('label_ranking_average_precision_score')\n",
        "        plt.title('Tree_giniIndex_Learning Curves')\n",
        "        plt.ylim(0, 2)\n",
        "\n",
        "tree_giniIndex = ClassifierChain(DecisionTreeClassifier(criterion='entropy', max_depth=15, class_weight=\"balanced\"))\n",
        "plot_learning_curves(tree_giniIndex, X, y)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8ab13a1b586b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskmultilearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_transform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifierChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_ranking_average_precision_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skmultilearn'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGvGtvZL5YKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#b) KneighborsClassifier\n",
        "# using classifier chains\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# initialize classifier chains multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "Knclassifier = ClassifierChain(KNeighborsClassifier())\n",
        "\n",
        "# train\n",
        "Knclassifier.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "y_predict_KN = Knclassifier.predict(X_test)\n",
        "\n",
        "accuracy_KN= accuracy_score(y_test,y_predict_KN)\n",
        "\n",
        "print(accuracy_KN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy5aJgyo5Y4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "426hEGx5bhpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using skmultilearn\n",
        "\n",
        "from skmultilearn.adapt import MLkNN\n",
        "\n",
        "classifier = MLkNN(k=20)\n",
        "\n",
        "# train\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# predict\n",
        "predictions = classifier.predict(X_test)\n",
        "\n",
        "accuracy_score(y_test,predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}